---
title: "Apprentissage de plongements de mots sur des corpus en langue de spécialité : une étude d’impact"
collection: publications
permalink: /publication/2020-06-08-43990Rencontre des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (RÉCITAL, 22e édition). Volume 3 : Rencontre des Étudiants Chercheurs en Informatique pour le TAL
excerpt: 'Les méthodes d’apprentissage de plongements lexicaux constituent désormais l’état de l’art pour la représentation du vocabulaire et des documents sous forme de vecteurs dans de nombreuses tâches de Traitement Automatique du Langage Naturel (TALN). Dans ce travail\, nous considérons l’apprentissage et l’usage de plongements lexicaux dans le cadre de corpus en langue de spécialité de petite taille. En particulier\, nous souhaitons savoir si dans ce cadre\, il est préférable d’utiliser des plongements préappris sur des corpus très volumineux tels Wikipédia ou bien s’il est préférable d’apprendre des plongements sur ces corpus en langue de spécialité. Pour répondre à cette question\, nous considérons deux corpus en langue de spécialité : OHSUMED issu du domaine médical\, et un corpus de documentation technique\, propriété de SNCF. Après avoir introduit ces corpus et évalué leur spécificité\, nous définissons une tâche de classification. Pour cette tâche\, nous choisissons d’utiliser en entrée d’un classifieur neuronal des représentations des documents qui sont soit basées sur des plongements appris sur les corpus de spécialité\, soit sur des plongements appris sur Wikipédia. Notre analyse montre que les plongements appris sur Wikipédia fournissent de très bons résultats. Ceux-ci peuvent être utilisés comme une référence fiable\, même si dans le cas d’OHSUMED\, il vaut mieux apprendre des plongements sur ce même corpus. La discussion des résultats se fait en interrogeant les spécificités des deux corpus\, mais ne permet pas d’établir clairement dans quels cas apprendre des plongements spécifiques au corpus.,Word embedding approaches are state of the art in Natural Language Processing (NLP). In this work\, we focus on learning word embeddings for small domain-specific corpora. In particular\, we would like to know whether word embeddings learnt over large corpora such as Wikipedia perform better than word embeddings learnt on domain specific corpora. In order to answer this question\, we consider two corpora : OHSUMED from the medical field\, and SNCF\, a technical documentation corpus. After presenting the corpora and evaluating their specificity\, we introduce a classification task. We use word embeddings learnt on domain-specific corpora or Wikipedia as input for this task. Our analysis demonstrates that word embeddings learnt on Wikipedia achieve excellent results\, even though\, in the case of OHSUMED\, domain specific word embeddings perform better.'
date: 2020-06-08
venue: 'Rencontre des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (RÉCITAL, 22e édition). Volume 3 : Rencontre des Étudiants Chercheurs en Informatique pour le TAL'
paperurl: 'https://hal.science/hal-02786198/document'
citation: '<i>Rencontre des Étudiants Chercheurs en Informatique pour le TAL</i>, Jun 2020, Nancy, France. pp.164-178'
---
Les méthodes d’apprentissage de plongements lexicaux constituent désormais l’état de l’art pour la représentation du vocabulaire et des documents sous forme de vecteurs dans de nombreuses tâches de Traitement Automatique du Langage Naturel (TALN). Dans ce travail\, nous considérons l’apprentissage et l’usage de plongements lexicaux dans le cadre de corpus en langue de spécialité de petite taille. En particulier\, nous souhaitons savoir si dans ce cadre\, il est préférable d’utiliser des plongements préappris sur des corpus très volumineux tels Wikipédia ou bien s’il est préférable d’apprendre des plongements sur ces corpus en langue de spécialité. Pour répondre à cette question\, nous considérons deux corpus en langue de spécialité : OHSUMED issu du domaine médical\, et un corpus de documentation technique\, propriété de SNCF. Après avoir introduit ces corpus et évalué leur spécificité\, nous définissons une tâche de classification. Pour cette tâche\, nous choisissons d’utiliser en entrée d’un classifieur neuronal des représentations des documents qui sont soit basées sur des plongements appris sur les corpus de spécialité\, soit sur des plongements appris sur Wikipédia. Notre analyse montre que les plongements appris sur Wikipédia fournissent de très bons résultats. Ceux-ci peuvent être utilisés comme une référence fiable\, même si dans le cas d’OHSUMED\, il vaut mieux apprendre des plongements sur ce même corpus. La discussion des résultats se fait en interrogeant les spécificités des deux corpus\, mais ne permet pas d’établir clairement dans quels cas apprendre des plongements spécifiques au corpus.,Word embedding approaches are state of the art in Natural Language Processing (NLP). In this work\, we focus on learning word embeddings for small domain-specific corpora. In particular\, we would like to know whether word embeddings learnt over large corpora such as Wikipedia perform better than word embeddings learnt on domain specific corpora. In order to answer this question\, we consider two corpora : OHSUMED from the medical field\, and SNCF\, a technical documentation corpus. After presenting the corpora and evaluating their specificity\, we introduce a classification task. We use word embeddings learnt on domain-specific corpora or Wikipedia as input for this task. Our analysis demonstrates that word embeddings learnt on Wikipedia achieve excellent results\, even though\, in the case of OHSUMED\, domain specific word embeddings perform better.

[Download paper here](https://hal.science/hal-02786198/document)

Recommended citation: <i>Rencontre des Étudiants Chercheurs en Informatique pour le TAL</i>, Jun 2020, Nancy, France. pp.164-178