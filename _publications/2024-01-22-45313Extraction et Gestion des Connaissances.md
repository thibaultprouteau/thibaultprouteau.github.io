---
title: "SINr-filtered : Favoriser l'émergence du sens en filtrant les communautés extraites des réseaux de cooccurrences de mots"
collection: publications
permalink: /publication/2024-01-22-45313Extraction et Gestion des Connaissances
excerpt: 'SINr. La représentation vectorielle du lexique est une problématique classique du traitement automatique du langage naturel. Les plongements lexicaux sont des vecteurs dans un espace où deux mots sémantiquement proches ont des représentations peu éloignées. Les méthodes telles que Word2vec utilisent des réseaux de neurones sur les cooccurrences des mots pour construire ces représentations. Les transformers ont démontré leur pertinence en contextualisant les vecteurs avec des architectures de plus en plus grandes. SINr (Sparse Interpretable Node Representations) est introduit par Prouteau et al. (2021) pour ne pas se concentrer uniquement sur des objectifs de performance\, mais pour entraîner des vecteurs de mots interprétables de façon frugale (Prouteau et al.\, 2022). Pour cela\, SINr s&apos;appuie sur un graphe de cooccurrences pondéré : les noeuds représentent les mots et sont connectés entre eux par des arêtes valuées en fonction de leur nombre de cooccurrences dans le corpus. Des communautés sont alors détectées sur le graphe de cooccurrences et utilisées comme dimensions de l&apos;espace latent : le vecteur d&apos;un mot est extrait en utilisant ses liens avec les communautés extraites. En accord avec l&apos;hypothèse distributionnelle\, Prouteau et al. (2021) considèrent que les mots qui distribuent leurs liens de manière similaire sur les communautés ont un sens similaire. Or\, si la méthode est à l&apos;état de l&apos;art de l&apos;interprétabilité tout en étant considérablement plus frugale que les approches concurrentes comme SPINE\, et que SINr obtient des performances en similarité identiques à SPINE\, celles-ci sont néanmoins légèrement inférieures à Word2vec.'
date: 2024-01-22
venue: 'Extraction et Gestion des Connaissances'
paperurl: 'https://hal.science/hal-04470451/document'
citation: '<i>Extraction et Gestion des Connaissances</i>, Jan 2024, Dijon, France. pp.429-430'
---
SINr. La représentation vectorielle du lexique est une problématique classique du traitement automatique du langage naturel. Les plongements lexicaux sont des vecteurs dans un espace où deux mots sémantiquement proches ont des représentations peu éloignées. Les méthodes telles que Word2vec utilisent des réseaux de neurones sur les cooccurrences des mots pour construire ces représentations. Les transformers ont démontré leur pertinence en contextualisant les vecteurs avec des architectures de plus en plus grandes. SINr (Sparse Interpretable Node Representations) est introduit par Prouteau et al. (2021) pour ne pas se concentrer uniquement sur des objectifs de performance\, mais pour entraîner des vecteurs de mots interprétables de façon frugale (Prouteau et al.\, 2022). Pour cela\, SINr s&apos;appuie sur un graphe de cooccurrences pondéré : les noeuds représentent les mots et sont connectés entre eux par des arêtes valuées en fonction de leur nombre de cooccurrences dans le corpus. Des communautés sont alors détectées sur le graphe de cooccurrences et utilisées comme dimensions de l&apos;espace latent : le vecteur d&apos;un mot est extrait en utilisant ses liens avec les communautés extraites. En accord avec l&apos;hypothèse distributionnelle\, Prouteau et al. (2021) considèrent que les mots qui distribuent leurs liens de manière similaire sur les communautés ont un sens similaire. Or\, si la méthode est à l&apos;état de l&apos;art de l&apos;interprétabilité tout en étant considérablement plus frugale que les approches concurrentes comme SPINE\, et que SINr obtient des performances en similarité identiques à SPINE\, celles-ci sont néanmoins légèrement inférieures à Word2vec.

[Download paper here](https://hal.science/hal-04470451/document)

Recommended citation: <i>Extraction et Gestion des Connaissances</i>, Jan 2024, Dijon, France. pp.429-430