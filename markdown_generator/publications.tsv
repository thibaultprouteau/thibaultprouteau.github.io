pub_date	title	venue	excerpt	citation	url_slug	paper_url	
2024-01-22	SINr-filtered : Favoriser l'émergence du sens en filtrant les communautés extraites des réseaux de cooccurrences de mots	Extraction et Gestion des Connaissances	SINr. La représentation vectorielle du lexique est une problématique classique du traitement automatique du langage naturel. Les plongements lexicaux sont des vecteurs dans un espace où deux mots sémantiquement proches ont des représentations peu éloignées. Les méthodes telles que Word2vec utilisent des réseaux de neurones sur les cooccurrences des mots pour construire ces représentations. Les transformers ont démontré leur pertinence en contextualisant les vecteurs avec des architectures de plus en plus grandes. SINr (Sparse Interpretable Node Representations) est introduit par Prouteau et al. (2021) pour ne pas se concentrer uniquement sur des objectifs de performance\, mais pour entraîner des vecteurs de mots interprétables de façon frugale (Prouteau et al.\, 2022). Pour cela\, SINr s'appuie sur un graphe de cooccurrences pondéré : les noeuds représentent les mots et sont connectés entre eux par des arêtes valuées en fonction de leur nombre de cooccurrences dans le corpus. Des communautés sont alors détectées sur le graphe de cooccurrences et utilisées comme dimensions de l'espace latent : le vecteur d'un mot est extrait en utilisant ses liens avec les communautés extraites. En accord avec l'hypothèse distributionnelle\, Prouteau et al. (2021) considèrent que les mots qui distribuent leurs liens de manière similaire sur les communautés ont un sens similaire. Or\, si la méthode est à l'état de l'art de l'interprétabilité tout en étant considérablement plus frugale que les approches concurrentes comme SPINE\, et que SINr obtient des performances en similarité identiques à SPINE\, celles-ci sont néanmoins légèrement inférieures à Word2vec.	<i>Extraction et Gestion des Connaissances</i>, Jan 2024, Dijon, France. pp.429-430	45313Extraction et Gestion des Connaissances	https://hal.science/hal-04470451/document	EGC-2024
2020-10-13	Technologies sémantiques et accès à l'information dans le prescrit SNCF	Congrès Lambda Mu 22 « Les risques au cœur des transitions » (e-congrès) - 22e Congrès de Maîtrise des Risques et de Sûreté de Fonctionnement, Institut pour la Maîtrise des Risques	Des expérimentations basées sur des technologies de traitement automatique du langage ont été menées au sein d'un programme de sécurité ferroviaire et de simplification documentaire afin d'améliorer la recherche d'information et la rédaction dans les textes de prescription SNCF.	<i>Congrès Lambda Mu 22 « Les risques au cœur des transitions » (e-congrès) - 22e Congrès de Maîtrise des Risques et de Sûreté de Fonctionnement, Institut pour la Maîtrise des Risques</i>, Oct 2020, Le Havre (e-congrès), France	44117Congrès Lambda Mu 22 « Les risques au cœur des transitions » (e-congrès) - 22e Congrès de Maîtrise des Risques et de Sûreté de Fonctionnement, Institut pour la Maîtrise des Risques	https://hal.science/hal-03476574/document	LAMBDAMU-2020
2023-11-28	Filtering communities in word co-occurrence networks to foster the emergence of meaning	Conference on Complex Networks and their Applications	With SINr\, we introduced a way to design graph and word embeddings based on community detection. Contrary to deep learning approaches\, this approach does not require much compute and was proven to be at the state-of-the-art for interpretability in the context of word embeddings. In this paper\, we investigate how filtering communities detected on word co-occurrence networks can improve performances of the approach. Community detection algorithms tend to uncover communities whose size follows a power-law distribution. Naturally\, the number of activations per dimensions in SINr follows a power-law: a few dimensions are activated by many words\, and many dimensions are activated by a few words. By filtering this distribution\, removing part of its head and tail\, we show improvement on intrinsic evaluation of the embedding while dividing their dimensionality by five. In addition\, we show that these results are stable through several runs\, thus defining a subset of distinctive features to describe a given corpus.	<i>Conference on Complex Networks and their Applications</i>, Nov 2023, Menton, France. pp.377-388, <a target="_blank" href="https://dx.doi.org/10.1007/978-3-031-53468-3_32">&#x27E8;10.1007/978-3-031-53468-3_32&#x27E9;</a>	45258Conference on Complex Networks and their Applications	https://hal.science/hal-04398742/document	CNA-2023
2022-06-21	Are Embedding Spaces Interpretable? Results of an Intrusion Detection Evaluation on a Large French Corpus	LREC 2022	Word embedding methods allow to represent words as vectors in a space that is structured using word co-occurrences so that words with close meanings are close in this space. These vectors are then provided as input to automatic systems to solve natural language processing problems. Because interpretability is a necessary condition to trusting such systems\, interpretability of embedding spaces\, the first link in the chain is an important issue. In this paper\, we thus evaluate the interpretability of vectors extracted with two approaches: SPINE\, a k-sparse auto-encoder\, and SINr\, a graph-based method. This evaluation is based on a Word Intrusion Task with human annotators. It is operated using a large French corpus\, and is thus\, as far as we know\, the first large-scale experiment regarding word embedding interpretability on this language. Furthermore\, contrary to the approaches adopted in the literature where the evaluation is performed on a small sample of frequent words\, we consider a more realistic use-case where most of the vocabulary is kept for the evaluation. This allows to show how difficult this task is\, even though SPINE and SINr show some promising results. In particular\, SINr results are obtained with a very low amount of computation compared to SPINE\, while being similarly interpretable.	<i>LREC 2022</i>, Jun 2022, Marseille, France. pp.4414-4419	44733LREC 2022	https://hal.science/hal-03770444/document	LREC-2022
2023-06-05	De l'interprétabilité des dimensions à l'interprétabilité du vecteur : parcimonie et stabilité	Conférence sur le Traitement Automatique des Langues Naturelles (TALN)	Les modèles d'apprentissage de plongements parcimonieux (SPINE\, SINr) ont pour objectif de produire un espace dont les dimensions peuvent être interprétées. Ces modèles visent des cas d'application critiques du traitement de la langue naturelle (e.g. usages médicaux ou judiciaires) et une utilisation des représentations dans le cadre des humanités numériques. Nous proposons de considérer non plus seulement l'interprétabilité des dimensions de l'espace de description\, mais celle des vecteurs de mots en eux-mêmes. Pour cela\, nous introduisons un cadre d'évaluation incluant le critère de stabilité\, et redénissant celui de la parcimonie en accord avec les théories psycholinguistiques. Tout d'abord\, les évaluations en stabilité indiquent une faible variabilité sur les modèles considérés. Ensuite\, pour redénir le critère de parcimonie\, nous proposons une méthode d'éparsication des vecteurs de plongements en gardant les composantes les plus fortement activées de chaque vecteur. Il apparaît que pour les deux modèles SPINE et SINr\, de bonnes performances en similarité sont permises par des vecteurs avec un très faible nombre de dimensions activées. Ces résultats permettent d'envisager l'interprétabilité de représentations éparses sans remettre en cause les performances.	<i>Conférence sur le Traitement Automatique des Langues Naturelles (TALN)</i>, Association pour le Traitement Automatique des Langues (ATALA), Jun 2023, Paris, France	45082Conférence sur le Traitement Automatique des Langues Naturelles (TALN)	https://hal.science/hal-04321436/document	TALN-2023
2023-05-31	SINr: a python package to train interpretable word and graph embeddings	French Regional Conference on Complex Systems	In this paper\, we introduce the SINr Python package to train word and graph embeddings. The SINr approach is based on community detection: a vector for a node is built upon the distribution of its connections through the communities detected on the graph at hand. Because of this\, the algorithm runs very fast\, and does not require GPUs to proceed. Furthermore\, the dimensions of the embedding space are interpretable\, those are based on the communities extracted. The package is distributed under Cecill-2.1 license and is available on Github and pypi.	<i>French Regional Conference on Complex Systems</i>, May 2023, Le Havre, France. pp.215, <a target="_blank" href="https://dx.doi.org/10.5281/zenodo.7957531">&#x27E8;10.5281/zenodo.7957531&#x27E9;</a>	45077French Regional Conference on Complex Systems	https://hal.science/hal-04113024/document	FRCCS-2023
2020-06-08	Apprentissage de plongements de mots sur des corpus en langue de spécialité : une étude d’impact	Rencontre des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (RÉCITAL, 22e édition). Volume 3 : Rencontre des Étudiants Chercheurs en Informatique pour le TAL	Les méthodes d’apprentissage de plongements lexicaux constituent désormais l’état de l’art pour la représentation du vocabulaire et des documents sous forme de vecteurs dans de nombreuses tâches de Traitement Automatique du Langage Naturel (TALN). Dans ce travail\, nous considérons l’apprentissage et l’usage de plongements lexicaux dans le cadre de corpus en langue de spécialité de petite taille. En particulier\, nous souhaitons savoir si dans ce cadre\, il est préférable d’utiliser des plongements préappris sur des corpus très volumineux tels Wikipédia ou bien s’il est préférable d’apprendre des plongements sur ces corpus en langue de spécialité. Pour répondre à cette question\, nous considérons deux corpus en langue de spécialité : OHSUMED issu du domaine médical\, et un corpus de documentation technique\, propriété de SNCF. Après avoir introduit ces corpus et évalué leur spécificité\, nous définissons une tâche de classification. Pour cette tâche\, nous choisissons d’utiliser en entrée d’un classifieur neuronal des représentations des documents qui sont soit basées sur des plongements appris sur les corpus de spécialité\, soit sur des plongements appris sur Wikipédia. Notre analyse montre que les plongements appris sur Wikipédia fournissent de très bons résultats. Ceux-ci peuvent être utilisés comme une référence fiable\, même si dans le cas d’OHSUMED\, il vaut mieux apprendre des plongements sur ce même corpus. La discussion des résultats se fait en interrogeant les spécificités des deux corpus\, mais ne permet pas d’établir clairement dans quels cas apprendre des plongements spécifiques au corpus.,Word embedding approaches are state of the art in Natural Language Processing (NLP). In this work\, we focus on learning word embeddings for small domain-specific corpora. In particular\, we would like to know whether word embeddings learnt over large corpora such as Wikipedia perform better than word embeddings learnt on domain specific corpora. In order to answer this question\, we consider two corpora : OHSUMED from the medical field\, and SNCF\, a technical documentation corpus. After presenting the corpora and evaluating their specificity\, we introduce a classification task. We use word embeddings learnt on domain-specific corpora or Wikipedia as input for this task. Our analysis demonstrates that word embeddings learnt on Wikipedia achieve excellent results\, even though\, in the case of OHSUMED\, domain specific word embeddings perform better.	<i>Rencontre des Étudiants Chercheurs en Informatique pour le TAL</i>, Jun 2020, Nancy, France. pp.164-178	43990Rencontre des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (RÉCITAL, 22e édition). Volume 3 : Rencontre des Étudiants Chercheurs en Informatique pour le TAL	https://hal.science/hal-02786198/document	RECITAL-2020
2024-12-23	From Communities to Interpretable Network and Word Embedding: an Unified Approach	Journal of Complex Networks	<div><p>Modeling information from complex systems such as humans social interaction or words co-occurrences in our languages can help to understand how these systems are organized and function. Such systems can be modeled by networks\, and network theory provides a useful set of methods to analyze them. Among these methods\, graph embedding is a powerful tool to summarize the interactions and topology of a network in a vectorized feature space. When used in input of machine learning algorithms\, embedding vectors help with common graph problems such as link prediction\, graph matching\, etc. In Natural Language Processing (NLP)\, such a vectorization process is also employed. Word embedding has the goal of representing the sense of words\, extracting it from large text corpora. Despite differences in the structure of information in input of embedding algorithms\, many graph embedding approaches are adapted and inspired from methods in NLP. Limits of these methods are observed in both domains. Most of these methods require long and resource greedy training. Another downside to most methods is that they are black-box\, from which understanding how the information is structured is rather complex. Interpretability of a model allows understanding how the vector space is structured without the need for external information\, and thus can be audited more easily. With both these limitations in mind\, we propose a novel framework to efficiently embed network vertices in an interpretable vector space. Our Lower Dimension Bipartite Framework (LDBGF) leverages the bipartite projection of a network using cliques to reduce dimensionality. Along with LDBGF\, we introduce two implementations of this framework that rely on communities instead of cliques: SINr-NR and SINr-MF. We show that SINr-MF can perform well on classical graphs and SINr-NR can produce high-quality graph and word embeddings that are interpretable and stable across runs.</p></div>	<i>Journal of Complex Networks</i>, 2024, 12 (6), <a target="_blank" href="https://dx.doi.org/10.1093/comnet/cnae034">&#x27E8;10.1093/comnet/cnae034&#x27E9;</a>	45649Journal of Complex Networks	https://hal.science/hal-04829653/document	JCN-2024
2023-06-20	Sparser is better: one step closer to word embedding interpretability	International Conference of Computational Semantics 2023 (IWCS)	Sparse word embeddings models (SPINE\, SINr) are designed to embed words in interpretable dimensions. An interpretable dimension is such that a human can interpret the semantic (or syntactic) relations between words active for a dimension. These models are useful for critical downstream tasks in natural language processing (e.g. medical or legal NLP)\, and digital humanities applications. This work extends interpretability at the vector level with a more manageable number of activated dimensions following recommendations from psycholinguistics. Subsequently\, one of the key criteria to an interpretable model is sparsity: in order to be interpretable\, not every word should be represented by all the features of the model\, especially if humans have to interpret these features and their relations. This raises one question: to which extent is sparsity sustainable with regard to performance? We thus introduce a sparsification procedure to evaluate its impact on two interpretable methods (SPINE and SINr) to tend towards sustainable vector interpretability. We also introduce stability as a new criterion to interpretability. Our stability evaluations show little albeit non-zero variation for SPINE and SINr embeddings. We then show that increasing sparsity does not necessarily interfere with performance. These results are encouraging and pave the way towards intrinsically interpretable word vectors.,Les modèles éparses de plongements lexicaux (SPINE\, SINr) ont pour objectif de plonger les mots dans des espaces de représentation aux dimensions interprétables. Une dimension est interprétable dès lors qu'un locuteur peut saisir la cohérence sémantique ou syntaxique des mots activés pour une dimension. Ces modèles sont utiles dans des cadres critiques d'utilisation du traitement automatique de la langue naturel (comme le TAL pour applications médicales ou juridiques)\, et pour des utilisations en humanités numériques. Ce travail vise à étendre la notion d'interprétabilité vers l'interprétabilité des vecteurs de mots en restreignant la quantité de dimensions à considérer depuis des contraintes psycholinguistiques. Ainsi\, l'éparsité (ou la parcimonie) des modèles interprétables est un point clé : pour garantir cette interprétabilité\, tous les mots ne doivent pas être représentés par toutes les dimensions du modèle\, particulièrement dans l'optique de la mise en relation de ces dimensions par des locuteurs. La question du compromis entre l'éparsité et les performances se pose donc. Nous introduisons une procédure d'éparsification pour évaluer son impact sur deux méthodes interprétables (SPINE et SINr) avec l'objectif de déplacer la notion d'interprétabilité vers les vecteurs de mots. Nous proposons également d'introduire la stabilité comme critère supplémentaire à l'interprétabilité. Nos évaluations de stabilité indique une variabilité faible quoi que non nulle pour les vecteurs de SPINE et de SINr. Nous montrons ensuite qu'une éparsité plus importante n'implique pas nécessairement de perte en performance. Ces résultats sont encourageants et participe de la construction de vecteurs de mots interprétables.	<i>International Conference of Computational Semantics 2023 (IWCS)</i>, Jun 2023, Nancy, France. pp.106-115	45097International Conference of Computational Semantics 2023 (IWCS)	https://hal.science/hal-04321407/document	IWCS-2023
2024-07-03	Graphs, Words, and Communities : converging paths to interpretability with a frugal embedding framework,	Thesis, Le Mans Université	Representation learning with word and graph embedding models allows distributed representations of information that can in turn be used in input of machine learning algorithms. Through the last two decades\, the tasks of embedding graphs’ nodes and words have shifted from matrix factorization approaches that could be trained in a matter of minutes to large models requiring ever larger quantities of training data and sometimes weeks on large hardware architectures. However\, in a context of global warming where sustainability is a critical concern\, we ought to look back to previous approaches and consider their performances with regard to resources consumption. Furthermore\, with the growing involvement of embeddings in sensitive machine learning applications (judiciary system\, health)\, the need for more interpretable and explainable representations has manifested. To foster efficient representation learning and interpretability\, this thesis introduces Lower Dimension Bipartite Graph Framework (LDBGF)\, a node embedding framework able to embed with the same pipeline graph data and text from large corpora represented as co-occurrence networks. Within this framework\, we introduce two implementations (SINr-NR\, SINr-MF) that leverage community detection in networks to uncover a latent embedding space where items (nodes/words) are represented according to their links to communities. We show that SINr-NR and SINr-MF can compete with similar embedding approaches on tasks such as predicting missing links in networks (link prediction) or node features (degree centrality\, PageRank score). Regarding word embeddings\, we show that SINr-NR is a good contender to represent words via word co-occurrence networks. Finally\, we demonstrate the interpretability of SINr-NR on multiple aspects. First with a human evaluation that shows that SINr-NR’s dimensions are to some extent interpretable. Secondly\, by investigating sparsity of vectors\, and how having fewer dimensions may allow interpreting how the dimensions combine and allow sense to emerge.,L'apprentissage de représentations au travers des méthodes de plongements de mots (word embedding) et de graphes (graph embedding) permet des représentations distribuées de l'information. Ces représentations peuvent à leur tour être utilisées en entrée d'algorithmes d'apprentissage automatique. Au cours des deux dernières décennies\, les tâches de plongement de nœuds et de mots sont passées d'approches par factorisation matricielle qui pouvaient être réalisées en quelques minutes à de grands modèles nécessitant des quantités toujours plus importantes de données d’apprentissage et parfois des semaines sur de grandes architectures matérielles. Toutefois\, dans un contexte de réchauffement climatique où la durabilité est une préoccupation essentielle\, il peut être souhaitable de revenir à des méthodes plus frugales comme elles pouvaient l’être par le passé. En outre\, avec l'implication croissante des plongements dans des applications sensibles de l’apprentissage automatique (système judiciaire\, santé)\, le besoin de représentations plus interprétables et explicables s'est manifesté. Pour favoriser l'apprentissage de représentations efficaces et l'interprétabilité\, cette thèse présente Lower Dimension Bipartite Graph Framework (LDBGF)\, un framework de plongements de nœuds capable d’extraire une représentation vectorielle avec le même pipeline de traitement\, à condition que les données proviennent d’un graphe ou de texte issu de grands corpus représentés sous forme de réseaux de cooccurrence. Dans ce cadre\, nous présentons deux implémentations (SINr- NR\, SINr-MF) qui tirent parti de la détection des communautés dans les réseaux pour découvrir un espace latent dans lequel les éléments (nœuds/mots) sont représentés en fonction de leurs liens avec les communautés. Nous montrons que SINr-NR et SINr-MF peuvent rivaliser avec des approches de plongements concurrentes sur des tâches telles que la prédiction des liens manquants dans les réseaux (link prediction) ou certaines caractéristiques des nœuds (centralité de degré\, score PageRank). En ce qui concerne les plongements de mots\, nous montrons que SINr-NR est un bon candidat pour représenter les mots en utilisant les réseaux de cooccurrences de mots. Enfin\, nous démontrons l'interprétabilité de SINr-NR sur plusieurs aspects. Tout d'abord\, une évaluation humaine montre que les dimensions de SINr-NR sont dans une certaine mesure interprétables. Ensuite\, nous étudions la parcimonie des vecteurs. Notamment\, combien un nombre réduit de dimensions peut permettre d'interpréter comment ces dernières se combinent et permettent de dégager un sens.	Machine Learning [cs.LG]. Le Mans Université, 2024. English. <a target="_blank" href="https://www.theses.fr/2024LEMA1006">&#x27E8;NNT : 2024LEMA1006&#x27E9;</a>	45476Thesis, Le Mans Université	https://theses.hal.science/tel-04696544/document	LMU-2024
2021-04-26	SINr: Fast Computing of Sparse Interpretable Node Representations is not a Sin!	Advances in Intelligent Data Analysis XIX, 19th International Symposium on Intelligent Data Analysis, IDA 2021	While graph embedding aims at learning low-dimensional representations of nodes encompassing the graph topology\, word embedding focus on learning word vectors that encode semantic properties of the vocabulary. The first finds applications on tasks such as link prediction and node classification while the latter is systematically considered in natural language processing. Most of the time\, graph and word embeddings are considered on their own as distinct tasks. However\, word co-occurrence matrices\, widely used to extract word embeddings\, can be seen as graphs. Furthermore\, most network embedding techniques rely either on a word embedding methodology (Word2vec) or on matrix factorization\, also widely used for word embedding. These methods are usually computationally expensive\, parameter dependant and the dimensions of the embedding space are not interpretable. To circumvent these issues\, we introduce the Lower Dimension Bipartite Graphs Framework (LDBGF) which takes advantage of the fact that all graphs can be described as bipartite graphs\, even in the case of textual data. This underlying bipartite structure may be explicit\, like in coauthor networks. However\, with LDBGF\, we focus on uncovering latent bipartite structures\, lying for instance in social or word co-occurrence networks\, and especially such structures providing conciser and interpretable representations of the graph at hand. We further propose SINr\, an efficient implementation of the LDBGF approach that extracts Sparse Interpretable Node Representations using community structure to approximate the underlying bipartite structure. In the case of graph embedding\, our near-linear time method is the fastest of our benchmark\, parameter-free and provides state-of-the-art results on the classical link prediction task. We also show that low-dimensional vectors can be derived from SINr using singular value decomposition. In the case of word embedding\, our approach proves to be very efficient considering the classical similarity evaluation.	<i>Advances in Intelligent Data Analysis XIX, 19th International Symposium on Intelligent Data Analysis, IDA 2021</i><a target="_blank" href="https://dx.doi.org/10.1007/978-3-030-74251-5_26">&#x27E8;10.1007/978-3-030-74251-5_26&#x27E9;</a>	44312Advances in Intelligent Data Analysis XIX, 19th International Symposium on Intelligent Data Analysis, IDA 2021	https://hal.science/hal-03197434/document	IDA-2021
